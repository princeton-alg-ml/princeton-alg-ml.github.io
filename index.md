---
layout: default
random_list: true
---

Welcome to the Princeton Alg-ML Seminar Page! 

Alg-ML is a weekly ML theory seminar primarily attended by the research groups of Prof. Sanjeev Arora, Prof. Elad Hazan, Prof. Jason Lee, and Prof. Chi Jin. 
We discuss exciting recent advancements in the areas of algorithm design and theoretical machine learning. 

Talks are held **every Monday from 12:15 pm ET to 1:15 pm ET**, with food usually served at 12:00 pm ET. It is open to all members of the Princeton community. 

For the 2023-2024 academic year, this seminar is organized by[^1]:
<ul id="namesList">
    <li>Simran Kaur</li>
    <li>Eshaan Nichani</li>
    <li>Jennifer Sun</li>
</ul>


If you would like to be notified about future Alg-ML talks, please subscribe to the [alg-ml mailing list](https://lists.cs.princeton.edu/mailman/listinfo/alg-ml-reading-group) and [google calendar](https://calendar.google.com/calendar/u/1?cid=Y185ZWQxMzVmOGMxN2JjZmNhYjAyOTk3ZGU0YTg0YzRhZDkyMjE1NTcwMGRhZjg1YjgzODJjZmUzNTBhNTk0MTQ3QGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20).

# This week's talk
## February 19, 2024 | Computer Science 302

> **Dhruv Rohatgi** 
>
> **A Computational Separation between Supervised Learning and RL**
>
> Supervised learning is an essential primitive in reinforcement learning (RL) over large state spaces. Indeed, there is a formal sense – namely, for the framework of block Markov Decision Processes (MDPs) – in which solving the former problem is a prerequisite for solving the latter. In terms of statistical complexity, there is little difference between these problems: the story of learning a near-optimal policy in Φ-realizable block MDPs (for a finite decoding function class Φ) mirrors the story of supervised learning (i.e., regression) over Φ. Both are solvable with roughly O( log |Φ|) samples. But the question of computational complexity is murkier, reflecting the unique challenges posed by RL. To what extent do the structural properties that enable computationally efficient supervised learning also enable computationally efficient RL? In this work, we construct a family of block MDPs where, under a plausible cryptographic hypothesis, reward-free RL is provably computationally harder than the natural, analogous regression problem. We also give a more direct proof that there is no oracle-efficient algorithm for RL using only the natural regression oracle. These results give evidence that some of the more complex oracles used in recent algorithms for RL in block MDPs may in fact be necessary, and separate the general problem of RL in block MDPs from various special cases. Based on work-in-progress with Noah Golowich and Ankur Moitra.

# Calendar
<!--<div class="responsive-iframe-container">
    <iframe src="https://calendar.google.com/calendar/embed?src=c_9ed135f8c17bcfcab02997de4a84c4ad922155700daf85b8382cfe350a594147%40group.calendar.google.com&ctz=America%2FNew_York" allowfullscreen></iframe>
</div>-->

<div style="position: relative; overflow: hidden; width: 100%; padding-top: 56.25%;">
    <iframe src="https://calendar.google.com/calendar/embed?src=c_9ed135f8c17bcfcab02997de4a84c4ad922155700daf85b8382cfe350a594147%40group.calendar.google.com&ctz=America%2FNew_York" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none;" allowfullscreen></iframe>
</div>
<!-- <iframe src="https://calendar.google.com/calendar/embed?src=c_9ed135f8c17bcfcab02997de4a84c4ad922155700daf85b8382cfe350a594147%40group.calendar.google.com&ctz=America%2FNew_York" style="border: 0" width="800" height="600" frameborder="0" scrolling="no"></iframe>
width=device-width -->


[^1]: The ordering on this page is randomized (as opposed to ordering alphabetically).  
