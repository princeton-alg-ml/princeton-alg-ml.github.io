---
layout: default
random_list: true
---

Welcome to the Princeton Alg-ML Seminar Page! 

Alg-ML is a weekly ML theory seminar primarily attended by the research groups of Prof. Sanjeev Arora, Prof. Elad Hazan, Prof. Jason Lee, and Prof. Chi Jin. 
We discuss exciting recent advancements in the areas of algorithm design and theoretical machine learning. 

Talks are held **every Monday from 12:30 pm ET to 1:30 pm ET**, with food usually served at 12:15 pm ET. It is open to all members of the Princeton community. 

For the 2023-2024 academic year, this seminar is organized by[^1]:
<ul id="namesList">
    <li>Simran Kaur</li>
    <li>Eshaan Nichani</li>
    <li>Jennifer Sun</li>
</ul>


If you would like to be notified about future Alg-ML talks, please subscribe to the [alg-ml mailing list](https://lists.cs.princeton.edu/mailman/listinfo/alg-ml-reading-group) and [google calendar](https://calendar.google.com/calendar/u/1?cid=Y185ZWQxMzVmOGMxN2JjZmNhYjAyOTk3ZGU0YTg0YzRhZDkyMjE1NTcwMGRhZjg1YjgzODJjZmUzNTBhNTk0MTQ3QGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20).

# This week's talk
## December 4, 2023 | Computer Science 302

> **Noam Razin** from Tel Aviv University
>
> **Two Theoretical Analyses of Modern Deep Learning: Graph Neural Networks and Language Model Finetuning**
>
> The resurgence of deep learning was largely driven by architectures conceived in the 20th century, trained using labeled data. In recent years, deep learning has undergone paradigm shifts characterized by new architectures and training regimes. Despite the popularity of the new paradigms, their theoretical understanding is limited. In this talk, I will present two recent works focusing on theoretical aspects of modern deep learning. The first work (to appear at NeurIPS 2023) considers the expressive power of graph neural networks, and formally quantifies their ability to model interactions between vertices. As a practical application of the theory, I will introduce a simple edge sparsification algorithm that achieves state-of-the-art results. The second work (under review) identifies a fundamental vanishing gradients problem that occurs when using reinforcement learning to finetune language models. I will demonstrate the detrimental effects of this phenomenon and present possible solutions. Lastly, I will conclude with an outlook on important questions raised by the advent of foundation models and possible tools for addressing them.
>
> Works covered in the talk were in collaboration with Nadav Cohen, Tom Verbin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, and Etai Littwin.

# Calendar
<!--<div class="responsive-iframe-container">
    <iframe src="https://calendar.google.com/calendar/embed?src=c_9ed135f8c17bcfcab02997de4a84c4ad922155700daf85b8382cfe350a594147%40group.calendar.google.com&ctz=America%2FNew_York" allowfullscreen></iframe>
</div>-->

<div style="position: relative; overflow: hidden; width: 100%; padding-top: 56.25%;">
    <iframe src="https://calendar.google.com/calendar/embed?src=c_9ed135f8c17bcfcab02997de4a84c4ad922155700daf85b8382cfe350a594147%40group.calendar.google.com&ctz=America%2FNew_York" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none;" allowfullscreen></iframe>
</div>
<!-- <iframe src="https://calendar.google.com/calendar/embed?src=c_9ed135f8c17bcfcab02997de4a84c4ad922155700daf85b8382cfe350a594147%40group.calendar.google.com&ctz=America%2FNew_York" style="border: 0" width="800" height="600" frameborder="0" scrolling="no"></iframe>
width=device-width -->


[^1]: The ordering on this page is randomized (as opposed to ordering alphabetically).  
